lambda x, y: x + y

broadcast variables - use efficiently send large, read-only values to workers (saved by workers for reuse)
    variableName = spark_context.broadcat(value)    # at the driver
    variableName.value  # at a worker

accumulators - aggregate values from workers to driver, only driver can access it's value (write-only for workers)
each task`s update to accumulator is guaranteed to be applied only once for actions, but have no guarantees for transformations
    accumulator = spark_xontext.accumulator(0)  # create at worker
    def f(x):
        global accumulator
        accumulator += x    # modify accumulator at a worker
    accumulator.value   # read at the driver

sc.parallelize(data, number_of_partitions) - creates RDD
sc.textFile("file", number_of_partitions) - creates RDD from file

transformations:    # transformations are evaluated lazily
    map(func)
    flatMap(func)
    filter(func)
    distinc()
    reduceByKey(func)
    groupByKey()    # prefer using reduceByKey transformation instead because it combines reults on partitions before shuffling them
    sortByKey()

actions:
    collect()
    reduce(func)    # func should be commutative and associative
    count()
    countByValue()  # returns a dictionary that maps values to counts
    forEach(func)
    take(n)
    takeOrdered(n, key=func)    # returns the list sorted in ascending order
    first()
    top(n)  # similar to takeOrdered() except that it returns the list in descending order

cache()     - caches RDDs;  if Spark runs out of memory, it will delete the least recently used (LRU) RDD first
unpersist() - inform Spark that you no longer need the RDD in memory

sc.createDataFrame(pandas_df)   - create Spark DataFrame from Pandas
spark_df.toPandas()             - convert Spark DataFrame to Pandas

rdd joins: join(anotherRdd), leftOuterJoin(anotherRdd), rightOuterJoin(anotherRdd), fullOuterJoin(anotherRdd)
