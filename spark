lambda x, y: x + y

broadcast variables - use efficiently send large, read-only values to workers (saved by workers for reuse)
    variableName = spark_context.broadcat(value)    # at the driver
    variableName.value  # at a worker

accumulators - aggregate values from workers to driver, only driver can access it's value (write-only for workers)
each task`s update to accumulator is guaranteed to be applied only once for actions, but have no guarantees for transformations
    accumulator = spark_xontext.accumulator(0)  # create at worker
    def f(x):
        global accumulator
        accumulator += x    # modify accumulator at a worker
    accumulator.value   # read at the driver

sc.parallelize(data, number_of_partitions) - creates RDD
sc.textFile("file", number_of_partitions) - creates RDD from file

transformations:
    map(func)
    filter(func)
    distinc()
    flatMap(func)
    reduceByKey(func)
    sortByKey()
    groupByKey()

actions:
    reduce(func)
    take(n)
    collect()
    takeOrdered(n, key=func)
    count()
    forEach(func)

cache() - save, don't recompute again
